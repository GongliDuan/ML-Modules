{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means is a machine learning algorithm used for $\\textbf{unsupervised learning}$ problem. (Simple to say, unsupervised learning is the machine learning task of inferring a function to describe hidden structure from unlabeled data.) Through K-Means algorithm, we could cluster similar data examples together automatically. In this note we will walk through this algorithm to understand how it works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, Choose K value\n",
    "\n",
    "The K value means how manys clusters you want to classify within your dataset. Because there is no label( or y) in dataset, we need to set K value by our experience and understanding on specific dataset. \n",
    "\n",
    "Besides that, there is a method could help us choose K value sometime, called \"elbow method\". It means we can plot cost( distance between each sample to its centroid) with various K-number, and if there is a significant decrease on some point, we can choose that value as K.\n",
    "\n",
    "\n",
    "2, Initial centroids\n",
    "\n",
    "After set K number, we can random pick K samples in dataset, and set them as inital centroids. \n",
    "\n",
    "\n",
    "3, Find the closest centroid\n",
    "\n",
    "As we know, we already have K initial centroids. And we need to assign each data point to the closest centroid. We could use square of difference between each data point and centroid, to represent distance(cost), and find the minimum. \n",
    "\n",
    "After this setp, we should have K clusters of data points.\n",
    "\n",
    "\n",
    "4, Compute means\n",
    "\n",
    "For each cluster, we use assigned data points to calculate mean and set it as new centroid for this cluster. \n",
    "\n",
    "\n",
    "5, Iteration\n",
    "\n",
    "we repeat step 3 and 4 for some times. By the iteration, we could find centroids move and each data point goes to corresponding cluster. \n",
    "\n",
    "\n",
    "Optional:\n",
    "\n",
    "We notice centroids are moving from initial to final position by interation. So there is chance we get local optima solution instead of the ideal. Therefore, in practice the K-means algorithm is usually run a few times with different random initializations, and by comparing total cost (find minimum) to get our final result.\n",
    "\n",
    "Noramlly, the bigger K we have, the less repetition, vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The following is the module built by python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class K_Means():\n",
    "    \n",
    "    def __init__(self,K):\n",
    "        # idx is a vector, dimension is (Number of sample * 1) \n",
    "        # which means cluster ID for each data point\n",
    "        self.K=K\n",
    "        self.min_cost=None\n",
    "        self.idx=None\n",
    "        \n",
    "        \n",
    "    def init_Centroids(self,X):\n",
    "        # Initial set K centroids randomly\n",
    "        \n",
    "        # m means number of sample\n",
    "        # n means number of feature      \n",
    "        m, n = X.shape\n",
    "        self.centroids = np.zeros([self.K,n])\n",
    "        randidx = np.random.permutation(range(m))\n",
    "        self.centroids = X[randidx[0:self.K]]\n",
    "        return self.centroids\n",
    "        \n",
    "        \n",
    "    def Find_Closest_Centroids(self,X):\n",
    "        # Assign each point to the closest centroid's cluster\n",
    "        m, n = X.shape\n",
    "        self.idx = np.zeros(m)\n",
    "        self.min_cost = np.zeros(m)\n",
    "        for i in range(0,m):\n",
    "            deltas = np.zeros(self.K)\n",
    "            sample = X[i]\n",
    "            for j in range(0,self.K):\n",
    "                k_point = self.centroids[j]\n",
    "                delta = sample - k_point\n",
    "                deltas[j] = np.dot(delta , delta)\n",
    "            self.idx[i] = np.argmin(deltas)\n",
    "            self.min_cost[i] = np.min(deltas)\n",
    "\n",
    "        \n",
    "    def Compute_Centroids(self,X):\n",
    "        # Compute the distance (cost)\n",
    "        m, n = X.shape\n",
    "        self.centroids = np.zeros([self.K, n])\n",
    "        \n",
    "        for k in range(0,self.K):\n",
    "            temp = np.where(self.idx==k)\n",
    "            self.centroids[k] = np.mean(X[np.where(self.idx==k)], axis=0)\n",
    "            \n",
    "        return self.centroids\n",
    "    \n",
    "    def predict(self,X,iterate_num=1000,repeat_num=10):\n",
    "        # do prediction\n",
    "        \n",
    "        # iterate_num is number the iteration times\n",
    "        # repeat_num is K-Means algorithm repeat times to avoid local optima solution\n",
    "        m, n = X.shape\n",
    "        cost = np.zeros(repeat_num)\n",
    "        result = np.zeros([repeat_num,m])\n",
    "        for i in range(repeat_num):\n",
    "            self.init_Centroids(X)\n",
    "            for j in range(iterate_num):\n",
    "                self.Find_Closest_Centroids(X)\n",
    "                self.Compute_Centroids(X)\n",
    "            cost[i] = sum(self.min_cost)\n",
    "            result[i] = self.idx\n",
    "        return result[np.argmin(cost)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
