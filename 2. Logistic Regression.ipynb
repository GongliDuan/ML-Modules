{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is also popular used just like linear regression. It's a basic module and good for solving machine learning classfication problems. \n",
    "\n",
    "Normally we can split it into two types, binary and one-vs-all (multiple) classfication. The theory behind them are pretty similar, for multiple probelms we can solve by milti-using binary module. In this notebook, it would walk through logistic regression module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, Define the notation:\n",
    "\n",
    "m-- Number of training samples\n",
    "\n",
    "n-- Number of sample features\n",
    "\n",
    "X-- Features\n",
    "\n",
    "$\\theta$-- Parameters\n",
    "\n",
    "y-- Targets\n",
    "\n",
    "$h\\tiny\\theta\\normalsize(x)$-- Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2, Hypothesis Function:\n"
   ]
  },
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3, Cost Function:\n",
    "\n",
    "\n",
    "Because there are only two values (0 and 1) for targets, so it's wisely to use log-loss to express the cost. Here is the cost function with regularization item:\n",
    "\n",
    "<img src=\"https://github.com/GongliDuan/exdata-data-household_power_consumption/blob/master/cost.png?raw=true\" alt=\"cost.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4, the Gradient Function:\n",
    "\n",
    "To minimize the cost function, which means the good $\\theta$ to fit the reality, we could use the gradient of cost function. \n"
    "\n",
   
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The following is the module built by python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "class Binary_Logistic_Regression():\n",
    "    \n",
    "    def __init__(self,lamda):\n",
    "        self.lamda = lamda\n",
    "        self.ave=None\n",
    "        self.sigma=None\n",
    "        self.coef=None\n",
    "    \n",
    "    \n",
    "    def norm_feature(self,X):\n",
    "        mu = np.mean(X, axis=0)\n",
    "        sigma = np.std(X, axis=0)\n",
    "        X_norm = (X - mu) / sigma\n",
    "        \n",
    "        return X_norm, mu, sigma\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def cost_function(self,theta, X, y):\n",
    "        m, n = X.shape\n",
    "        y = y.reshape((m, 1))\n",
    "        theta = theta.reshape((n, 1))\n",
    "        z = X.dot(theta)\n",
    "        h_x = self.sigmoid(z)\n",
    "        J = -1.0 / m * (y.T.dot(np.log(h_x)) + (1 - y).T.dot(np.log(1 - h_x))) \\\n",
    "            + self.lamda / (2.0 * m) * sum(theta[1:]**2)\n",
    "        return J\n",
    "    \n",
    "    def gradenit_function(self,theta, X, y):\n",
    "        \n",
    "        m, n = X.shape\n",
    "        y = y.reshape((m, 1))\n",
    "        theta = theta.reshape((n, 1))\n",
    "        z = X.dot(theta)\n",
    "        h_x = self.sigmoid(z)\n",
    "        grad = np.zeros((n, 1))\n",
    "        grad[0] = 1.0 / m * sum(h_x - y)\n",
    "        grad[1:] = 1.0 / m * X[:, 1:].T.dot(h_x - y) + float(self.lamda) / m * theta[1:]\n",
    "        return grad.ravel()\n",
    "    \n",
    "    def train(self,X,y):\n",
    "        m, n = X.shape\n",
    "        X, self.mu, self.sigma = self.norm_feature(X)\n",
    "        X = np.c_[np.ones((m, 1)), X]\n",
    "        theta = np.zeros(X.shape[1])\n",
    "        result = scipy.optimize.minimize(fun=self.cost_function, x0=theta, args=(X, y),\n",
    "                                         method='BFGS', jac=self.gradenit_function,\n",
    "                                         options={\"maxiter\": 100, \"disp\": False})\n",
    "\n",
    "        self.coef = result.x\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def predict_prob(self,X):\n",
    "        m, n = X.shape\n",
    "        X = np.c_[np.ones((m, 1)), (X - self.mu) / self.sigma]\n",
    "        y_prob = self.sigmoid(X.dot(self.coef.reshape((n+1, 1))))\n",
    "\n",
    "        return y_prob.ravel()\n",
    "    \n",
    "    def predict(self,X):\n",
    "        prob = self.predict_prob(X)\n",
    "        prob[prob <= 0.5] = 0\n",
    "        prob[prob > 0.5] = 1\n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### One-vs-all Logistic Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type algorithm is used to solve the multi-class classfication probelms. As I mentioned before, it could be implemented by multi-used binary logistic module which we just built. We can set a loop for all classes, each time we set one class as 1 and all the others as 0, then we can predict the probability of 1. When the loop is done, we get the class-probability matrix, and the class with the highest probability will be the final result of calssification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The following is the module built by python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Onevsall_Logistic_Regression():\n",
    "    \n",
    "    def __init__(self, lamda):\n",
    "        self.lamda=lamda\n",
    "        self.value=None\n",
    "        self.num_samples=None\n",
    "        self.num_classes=None\n",
    "        self.theta_matrix=[]\n",
    "        self.prob_matrix=None\n",
    "        \n",
    "    def train(self,X,y):\n",
    "        \n",
    "        self.value=np.unique(y)\n",
    "        self.num_samples=len(X)\n",
    "        self.num_classes=len(self.value)\n",
    "        \n",
    "        for each in self.value:\n",
    "            temp_y=np.array(y)\n",
    "            \n",
    "            for i in range(0,self.num_samples):\n",
    "                if y[i]==each:\n",
    "                    temp_y[i]=1\n",
    "                else: temp_y[i]=0\n",
    "            print temp_y\n",
    "                               \n",
    "            a=Binary_Logistic_Regression(lamda=self.lamda)\n",
    "            self.theta_matrix.append(a.train(X,temp_y))\n",
    "        return self\n",
    "    \n",
    "    def predict_prob(self,X):\n",
    "    \n",
    "        self.prob_matrix = np.empty((X.shape[0], self.num_classes))    \n",
    "        for k, a in enumerate(self.theta_matrix):\n",
    "            self.prob_matrix[:, k] = a.predict_prob(X)\n",
    "    \n",
    "            \n",
    "    def predict(self,X):\n",
    "        self.predict_prob(X)\n",
    "        y_predict_idx = np.argmax(self.prob_matrix, axis=1)\n",
    "        y_predict = np.array([self.value[i] for i in y_predict_idx])\n",
    "        return y_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
