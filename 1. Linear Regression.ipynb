{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my view, liner Regression may be the most widely used in sloving machine learning regression problems. It's strightforward, and normally could get pretty good result. In this notebook, it would walk through this algorithm and build a math module with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, Define the notation:\n",
    "\n",
    "m-- Number of training samples\n",
    "\n",
    "n-- Number of sample features\n",
    "\n",
    "X-- Features\n",
    "\n",
    "$\\theta$ -- Parameters\n",
    "\n",
    "y-- Targets\n",
    "\n",
    "$h\\tiny\\theta\\normalsize(x)$-- Hypothesis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2, Cost Function:\n",
    "\n",
    "The cost function is:\n",
    "\n",
    "J(Î¸)=\n",
    "\n",
    "The second part of the equation is 'regularization term'. Sometimes it could be neglected, but it had better to keep, because by it we can control the hurt of overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3, the Gradient Function:\n",
    "\n",
    "\n",
    "We would like to minimize the cost function by finding the right $\\theta$. And when we know cost function and gradient function, we can apply Gradient Descent or other more advanced optimization algorithms to learn the parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The following is the module built by python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "class Linear_Regression():\n",
    "    \n",
    "    def __init__(self, lamda):\n",
    "        \n",
    "        self.lamda = lamda\n",
    "        self.ave = None\n",
    "        self.sigma = None\n",
    "        self.coef = None\n",
    "\n",
    "    def feature_norm(self, X):\n",
    "\n",
    "        # Normalize all features to speed up the gradient descent process\n",
    "        ave = np.mean(X, axis=0)\n",
    "        sigma = np.std(X, axis=0)\n",
    "        X_norm = (X - ave) / sigma\n",
    "\n",
    "        return X_norm, ave, sigma\n",
    "    \n",
    "    def calc_cost(self, theta, X, y):\n",
    "        \n",
    "        # Formulate cost function\n",
    "        m, n = X.shape\n",
    "        y = y.reshape((m, 1))\n",
    "        theta = theta.reshape((n, 1))\n",
    "        J = 1.0 / (2 * m) * sum((X.dot(theta) - y)**2) \\\n",
    "            + self.lamda / (2.0 * m) * sum(theta[1:]**2)\n",
    "\n",
    "        return J\n",
    "    \n",
    "    def calc_gradient(self, theta, X, y):\n",
    "\n",
    "        # Formulate the gradient of the cost function\n",
    "        m, n = X.shape\n",
    "        y = y.reshape((m, 1))\n",
    "        theta = theta.reshape((n, 1))\n",
    "        grad = np.zeros((n, 1))\n",
    "        grad[0] = 1.0 / m * sum(X.dot(theta) - y)\n",
    "        grad[1:] = 1.0 / m * X[:, 1:].T.dot(X.dot(theta) - y) \\\n",
    "                   + float(self.lamda) / m * theta[1:]\n",
    "\n",
    "        return grad.ravel()\n",
    "    \n",
    "    def train(self, X, y):\n",
    "\n",
    "        # Fit the model\n",
    "        m, n = X.shape\n",
    "        X, self.mu, self.sigma = self.feature_norm(X)\n",
    "        X = np.c_[np.ones((m, 1)), X]\n",
    "        theta = np.zeros(X.shape[1])\n",
    "        result = scipy.optimize.minimize(fun=self.calc_cost, x0=theta, args=(X, y),\n",
    "                                         method='BFGS', jac=self.calc_gradient,\n",
    "                                         options={\"maxiter\": 100, \"disp\": False})\n",
    "        \n",
    "        self.coef = result.x\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # predict with the fitted model\n",
    "        m, n = X.shape\n",
    "        X = np.c_[np.ones((m, 1)), (X - self.mu) / self.sigma]\n",
    "        y_pred = X.dot(self.coef.reshape((n+1, 1)))\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
